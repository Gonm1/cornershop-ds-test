{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "from joblib import dump\n",
    "import sys\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split data into train and test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.read_csv(\"data.csv\", index_col=\"order_id\")\n",
    "\n",
    "y = data.pop(\"total_minutes\").to_numpy()\n",
    "X = data.to_numpy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define pipelines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pipeline composition\n",
    "1. Standard scaler -> standardizes the data with mean and std\n",
    "2. PCA -> Principal component analysis (change of basis on the input data)\n",
    "3. ML Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svm_pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"pca\", PCA()), (\"svm\", SVR())])\n",
    "rfr_pipeline = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), (\"pca\", PCA()), (\"rfr\", RandomForestRegressor())]\n",
    ")\n",
    "xgb_pipeline = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), (\"pca\", PCA()), (\"xgb\", XGBRegressor())]\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Randomized search"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A random search is performed to find the best hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svm_params = {\n",
    "    \"pca__n_components\": [5, 10, 15, 20, 25, 30, 35, 37],\n",
    "    \"svm__C\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 30, 32, 35],\n",
    "    \"svm__epsilon\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    \"svm__gamma\": [\"scale\", \"auto\"],\n",
    "    \"svm__kernel\": [\"linear\", \"sigmoid\", \"rbf\"],\n",
    "    \"svm__shrinking\": [True, False],\n",
    "}\n",
    "\n",
    "rfr_params = {\n",
    "    \"pca__n_components\": [5, 10, 15, 20, 25, 30, 35, 37],\n",
    "    \"rfr__n_estimators\": [\n",
    "        100,\n",
    "        200,\n",
    "        300,\n",
    "        400,\n",
    "        500,\n",
    "        600,\n",
    "        700,\n",
    "        800,\n",
    "        900,\n",
    "        1000,\n",
    "        1100,\n",
    "        1200,\n",
    "        1300,\n",
    "        1400,\n",
    "        1500,\n",
    "        1600,\n",
    "        1700,\n",
    "        1800,\n",
    "        1900,\n",
    "    ],\n",
    "    \"rfr__max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"rfr__max_depth\": [10, 20, 30, 40, 50, 60, 70, 80, 90, None],\n",
    "    \"rfr__min_samples_split\": [2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    \"rfr__min_samples_leaf\": [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    \"rfr__bootstrap\": [True, False],\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    \"pca__n_components\": [5, 10, 15, 20, 25, 30, 35, 37],\n",
    "    \"xgb__objective\": [\"reg:squarederror\", \"reg:squaredlogerror\"],\n",
    "    \"xgb__learning_rate\": [0.01, 0.005, 0.001],\n",
    "    \"xgb__max_depth\": [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"xgb__min_child_weight\": [3, 4, 5, 6, 7, 8, 9],\n",
    "    \"xgb__subsample\": [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    \"xgb__colsample_bytree\": [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    \"xgb__n_estimators\": [\n",
    "        100,\n",
    "        200,\n",
    "        300,\n",
    "        400,\n",
    "        500,\n",
    "        600,\n",
    "        700,\n",
    "        800,\n",
    "        900,\n",
    "        1000,\n",
    "        1100,\n",
    "        1200,\n",
    "        1300,\n",
    "        1400,\n",
    "        1500,\n",
    "        1600,\n",
    "        1700,\n",
    "        1800,\n",
    "        1900,\n",
    "    ],\n",
    "    \"xgb__n_jobs\": [2],\n",
    "}\n",
    "\n",
    "\n",
    "svm_random_search = RandomizedSearchCV(\n",
    "    estimator=svm_pipeline,\n",
    "    param_distributions=svm_params,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    refit=\"neg_mean_absolute_error\",\n",
    "    cv=3,\n",
    "    n_iter=2000,\n",
    "    random_state=0,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "rfr_random_search = RandomizedSearchCV(\n",
    "    estimator=rfr_pipeline,\n",
    "    param_distributions=rfr_params,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    refit=\"neg_mean_absolute_error\",\n",
    "    n_iter=2000,\n",
    "    cv=3,\n",
    "    random_state=0,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_pipeline,\n",
    "    param_distributions=xgb_params,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    refit=\"neg_mean_absolute_error\",\n",
    "    cv=3,\n",
    "    n_iter=2000,\n",
    "    random_state=0,\n",
    "    n_jobs=6,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"svm started random search\")\n",
    "svm_random_search.fit(x_train, y_train)\n",
    "print(\"random forest started random search\")\n",
    "rfr_random_search.fit(x_train, y_train)\n",
    "print(\"xgb regressor started random search\")\n",
    "xgb_random_search.fit(x_train, y_train)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Best scores on training data\n",
    "print(f\"svm: {svm_random_search.best_score_:.3f}\")\n",
    "print(f\"rfr: {rfr_random_search.best_score_:.3f}\")\n",
    "print(f\"xgb: {xgb_random_search.best_score_:.3f}\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# mae and mse on test data\n",
    "\n",
    "original_stdout = sys.stdout  # Save a reference to the original standard output\n",
    "\n",
    "with open(\"rand_search_results/results.txt\", \"w\") as f:\n",
    "    sys.stdout = f  # Change the standard output to the file we created.\n",
    "\n",
    "    print(\"mae and mse on test data\")\n",
    "    # Mean absolute error\n",
    "    print(\n",
    "        f\"mae svm: {mean_absolute_error(y_true=y_test, y_pred=svm_random_search.best_estimator_.predict(x_test)):02f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"mae rfr: {mean_absolute_error(y_true=y_test, y_pred=rfr_random_search.best_estimator_.predict(x_test)):02f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"mae xgb: {mean_absolute_error(y_true=y_test, y_pred=xgb_random_search.best_estimator_.predict(x_test)):02f}\"\n",
    "    )\n",
    "\n",
    "    # Mean squared error\n",
    "    print(\n",
    "        f\"mse svm: {mean_squared_error(y_true=y_test, y_pred=svm_random_search.best_estimator_.predict(x_test)):02f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"mse rfr: {mean_squared_error(y_true=y_test, y_pred=rfr_random_search.best_estimator_.predict(x_test)):02f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"mse xgb: {mean_squared_error(y_true=y_test, y_pred=xgb_random_search.best_estimator_.predict(x_test)):02f}\"\n",
    "    )\n",
    "\n",
    "    sys.stdout = original_stdout  # Reset the standard output to its original value\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Model saving\n",
    "dump(svm_random_search, \"rand_search_results/svm_random_search\")\n",
    "dump(svm_random_search.best_estimator_, \"rand_search_results/svm_best_model\")\n",
    "dump(rfr_random_search, \"rand_search_results/rfr_random_search\")\n",
    "dump(rfr_random_search.best_estimator_, \"rand_search_results/rfr_best_model\")\n",
    "dump(xgb_random_search, \"rand_search_results/xgb_random_search\")\n",
    "dump(xgb_random_search.best_estimator_, \"rand_search_results/xgb_best_model\")\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('default': venv)"
  },
  "interpreter": {
   "hash": "612fb3d9ffff20ebf687db91124ed6b2b9e249002eae363a2927c131d0f32fd3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}