{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data processing notebook\n",
    "author: Gonzalo Miranda Cabrera\n",
    "\n",
    "objective: create a clean dataset with the provided data tables for later training of ml models.\n",
    "\n",
    "summary:\n",
    "1. cleaning tables: process each table and get it ready for a join into one dataset.\n",
    "2. join tables: combine the tables into one.\n",
    "3. outliers: train an isolation forest for anomaly detection.\n",
    "4. features: add more features to the dataset. (correlation matrix and rates between preexisting columns)\n",
    "5. Split Nan for submission: save rows with total_minutes with nan values for submission file.\n",
    "6. write csv: save data and submission to disk.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seaborn import heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.distance import distance\n",
    "from sklearn.ensemble import IsolationForest\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "order_products = pd.read_csv('data/order_products.csv')\n",
    "orders = pd.read_csv('data/orders.csv')\n",
    "shoppers = pd.read_csv('data/shoppers.csv')\n",
    "storebranch = pd.read_csv('data/storebranch.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleaning tables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### order_products table\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "order_products.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the provided orders_preducts table we can extract the following:\n",
    "- total units per order\n",
    "- total kg per order\n",
    "- total unique products per order"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Aggregate total units and total kgs per order\n",
    "total_order_products = (\n",
    "    order_products.groupby([\"order_id\", \"buy_unit\"]).sum().reset_index()\n",
    ")\n",
    "total_order_products.head(7)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate the amount of unique products per order\n",
    "unique_products = (\n",
    "    order_products.groupby([\"order_id\"])['product_id'].count().reset_index()\n",
    ")\n",
    "unique_products.head(7)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Leave only unique order_id on order_products table\n",
    "order_products.drop_duplicates(subset=['order_id'], inplace=True)\n",
    "order_products.drop(columns=['product_id', 'quantity', 'buy_unit'], inplace=True)\n",
    "order_products.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Add unique products per order to order_products table\n",
    "order_products = (\n",
    "    order_products.set_index(\"order_id\")\n",
    "    .join(unique_products.set_index(\"order_id\"))\n",
    "    .reset_index()\n",
    ")\n",
    "order_products.rename(columns={\"product_id\": \"unique_products\"}, inplace=True)\n",
    "order_products.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Add total units per order_id\n",
    "filter_ = total_order_products[\"buy_unit\"] == \"UN\"\n",
    "order_products = (\n",
    "    order_products.set_index(\"order_id\")\n",
    "    .join(total_order_products[filter_][[\"order_id\", \"quantity\"]].set_index(\"order_id\"))\n",
    "    .reset_index()\n",
    ")\n",
    "order_products.rename(columns={\"quantity\": \"units\"}, inplace=True)\n",
    "order_products.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Add total kgs per order_id\n",
    "filter_ = total_order_products[\"buy_unit\"] == \"KG\"\n",
    "order_products = (\n",
    "    order_products.set_index(\"order_id\")\n",
    "    .join(total_order_products[filter_][[\"order_id\", \"quantity\"]].set_index(\"order_id\"))\n",
    "    .reset_index()\n",
    ")\n",
    "order_products.rename(columns={\"quantity\": \"kgs\"}, inplace=True)\n",
    "order_products.head(30)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fill NaN values of order_products table\n",
    "order_products.fillna(0, inplace=True)\n",
    "\n",
    "del filter_, total_order_products, unique_products\n",
    "\n",
    "order_products.head(30)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Orders table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "orders.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the orders table we can do the following:\n",
    "- obtain day of the week that the order took place from promised_time\n",
    "- convert lat and lng to a point to calculate distance with store\n",
    "- convert on_demand to int"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "orders.isna().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check the days, month and years that the orders took place\n",
    "orders['promised_time'] = pd.to_datetime(orders['promised_time'])\n",
    "orders.promised_time.dt.day.unique(), orders.promised_time.dt.month.unique(), orders.promised_time.dt.year.unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create one-hot encoded columns for the days\n",
    "orders['day'] = orders.promised_time.dt.day_name()\n",
    "orders = pd.concat(\n",
    "    [orders, pd.get_dummies(orders.day, prefix=\"is\")], axis=1\n",
    ")\n",
    "orders.drop(columns=['promised_time', 'day'], inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create delivery_point for distance calculation with stores\n",
    "orders[\"delivery_point\"] = list(zip(orders.lat, orders.lng))\n",
    "orders.drop(columns=[\"lat\", \"lng\"], inplace=True)\n",
    "\n",
    "# Covnert bool to int\n",
    "orders[\"on_demand\"] = orders[\"on_demand\"].astype(int)\n",
    "orders.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Shoppers table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shoppers.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "from the shoppers table we can do:\n",
    "- change seniority to one-hot encoding\n",
    "- fill NaN with mean"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shoppers.isna().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fill NaN with mean of each column\n",
    "shoppers.fillna(shoppers.mean(), inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shoppers.isna().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Convert seniority categorical to one hot encoding\n",
    "shoppers = pd.concat(\n",
    "    [shoppers, pd.get_dummies(shoppers.seniority, prefix=\"seniority\")], axis=1\n",
    ")\n",
    "shoppers.drop(columns=[\"seniority\"], inplace=True)\n",
    "\n",
    "shoppers.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Storebranch table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "storebranch.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create store_point for distance calculation with orders\n",
    "storebranch[\"store_point\"] = list(zip(storebranch[\"lat\"], storebranch[\"lng\"]))\n",
    "storebranch.drop(columns=[\"lat\", \"lng\", \"store_id\"], inplace=True)\n",
    "storebranch.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Join tables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We join all the above tables into the data variable"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "orders.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Join orders and storebranch tables into data\n",
    "data = (\n",
    "    orders.set_index(\"store_branch_id\")\n",
    "    .join(storebranch.set_index(\"store_branch_id\"))\n",
    "    .reset_index()\n",
    ")\n",
    "data.drop(columns=[\"store_branch_id\"], inplace=True)\n",
    "data.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate distance from delivery_point to store_point\n",
    "data[\"distance\"] = data[[\"delivery_point\", \"store_point\"]].apply(\n",
    "    lambda values: distance(values[0], values[1]).kilometers, axis=1\n",
    ")\n",
    "data.drop(columns=[\"delivery_point\", \"store_point\"], inplace=True)\n",
    "data.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Join data and shoppers tables\n",
    "data = (\n",
    "    data.set_index(\"shopper_id\").join(shoppers.set_index(\"shopper_id\")).reset_index()\n",
    ")\n",
    "data.drop(columns=[\"shopper_id\"], inplace=True)\n",
    "data.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Join data and order_products tables\n",
    "data = data.set_index(\"order_id\").join(order_products.set_index(\"order_id\"))\n",
    "data.head()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check for NaN values\n",
    "data.isna().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# order_ids in orders but not in order_products\n",
    "data.drop(\n",
    "    data[\n",
    "        (data[\"units\"].isna() | data[\"kgs\"].isna()) | data[\"unique_products\"].isna()\n",
    "    ].index,\n",
    "    inplace=True,\n",
    ")\n",
    "data.isna().sum()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# show data\n",
    "data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# delete unused variables\n",
    "del storebranch, shoppers, order_products, orders"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Outliers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Split nan for isolation forest training\n",
    "as isolation forest can not be trained if dataset has NaN values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission = data[pd.isna(data['total_minutes'])]\n",
    "data.dropna(inplace=True)\n",
    "data.shape, submission.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Isolation forest training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "isolation_forest = IsolationForest(\n",
    "    max_features=1.0,\n",
    "    contamination=0.05,\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "data[\"anomaly_label\"] = isolation_forest.fit_predict(data)\n",
    "data[data[\"anomaly_label\"] == -1]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Drop found anomalies\n",
    "anomalies are set with a label of -1, so we select all the labels that are 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = data[data[\"anomaly_label\"] == 1]\n",
    "data = data.drop(columns=['anomaly_label'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Merge data for features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = pd.concat([data, submission])\n",
    "data.isna().sum()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "to make more features we use the correlation matrix (pearson) to find the most correlated with total_minutes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot heatmap\n",
    "plt.figure(figsize=(20, 20))\n",
    "heatmap(\n",
    "    data.corr(), annot=True, linewidth=0.8, mask=np.triu(data.corr()), cmap=\"RdYlBu_r\"\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "unique_products, units and kgs are the features that have greater correlation with total_minutes.\n",
    "\n",
    "so we create the following features:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create estimated order size assuming 1 kg is 1 unit\n",
    "data[\"order_size\"] = data.units + data.kgs\n",
    "\n",
    "# scale most correlated features with math functions\n",
    "data[\"root_order_size\"] = np.sqrt(data.order_size)\n",
    "data[\"sqrd_order_size\"] = data.order_size ** 2\n",
    "data[\"logn_order_size\"] = np.log(data.order_size)\n",
    "\n",
    "data[\"root_units\"] = np.sqrt(data.units)\n",
    "data[\"sqrd_units\"] = data.units ** 2\n",
    "data[\"logn_units\"] = np.log(data.units + 1)\n",
    "\n",
    "data[\"root_kgs\"] = np.sqrt(data.kgs)\n",
    "data[\"sqrd_kgs\"] = data.kgs ** 2\n",
    "data[\"logn_kgs\"] = np.log(data.kgs + 1)\n",
    "\n",
    "data[\"root_unique_products\"] = np.sqrt(data.unique_products)\n",
    "data[\"sqrd_unique_products\"] = data.unique_products ** 2\n",
    "data[\"logn_unique_products\"] = np.log(data.unique_products)\n",
    "\n",
    "# Create different rates with distance as it has more correlation than other features\n",
    "data[\"distance_div_units\"] = np.true_divide(data.distance, data.units + 1)\n",
    "data[\"distance_div_kgs\"] = np.true_divide(data.distance, data.kgs + 1)\n",
    "data[\"distance_div_unique_products\"] = data.distance / data.unique_products\n",
    "data[\"distance_div_order_size\"] = data.distance / data.order_size\n",
    "\n",
    "# Create different rates with diferent amounts and picking speed that make a sense of time.\n",
    "data[\"unique_products_div_picking_speed\"] = data.unique_products / data.picking_speed\n",
    "# For example we are dividing products by products/minute and we end up with minutes.\n",
    "\n",
    "data[\"units_div_picking_speed\"] = data.units / data.picking_speed\n",
    "data[\"kgs_div_picking_speed\"] = data.kgs / data.picking_speed\n",
    "data[\"order_size_div_picking_speed\"] = data.order_size / data.picking_speed\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot the heatmap again to see the correlation of the newly added features\n",
    "plt.figure(figsize=(30, 30))\n",
    "heatmap(\n",
    "    data.corr(), annot=True, linewidth=0.8, mask=np.triu(data.corr()), cmap=\"RdYlBu_r\"\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show data.min and data.max to see if there are some undefined values\n",
    "data.min(), data.max()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "no undefined values are found"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split NaN for later submission"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "submission = data[pd.isna(data['total_minutes'])]\n",
    "data.dropna(inplace=True)\n",
    "data.shape, submission.shape\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save data and submission to disk\n",
    "data.to_csv('data.csv')\n",
    "submission.to_csv('submission_data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('default': venv)"
  },
  "interpreter": {
   "hash": "612fb3d9ffff20ebf687db91124ed6b2b9e249002eae363a2927c131d0f32fd3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}